---

# Define a list of SLURM partitions to create.
openhpc_slurm_partitions: 
   - "{{ cluster.compute }}"

# Choose between beegfs, nfs and none.
cluster_fs: nfs

openhpc_slurm_conf:
  location: "{{ alaska_homedir }}/slurm/slurm.conf"
  shared_fs: true

# Config for Ceph mount
ceph_mount_enabled: false
ceph_mount_fuse: false

# BeegFS config
beegfs_state: present
beegfs_force_format: no
beegfs_interfaces:
- ib0
beegfs_mgmt_host: "{{ groups['cluster_beegfs_mgmt'] | first }}"
#- dev: /dev/md0
beegfs_oss:
- path: /data/beegfs/beegfs_oss/path
  port: 8003
beegfs_client:
- path: "/mnt/{{ cluster_name }}"
  port: 8004
# - path: "/mnt/storage-ssd"
- path: "{{ alaska_homedir }}"
  port: 18004
  mgmt_host: "storage-ssd-node-0"
- path: "/mnt/storage-nvme"
  port: 28004
  mgmt_host: "storage-nvme-node-0"

# # Software defined raid config
# md0:
#   # Define array name
#   name: 'md0'
#   # Define disk devices to assign to array
#   devices:
#     - '/dev/sdb'
#     - '/dev/sdc'
#     # - '/dev/sdd' this has failed on one of the nodes so ignore it!
#   # Define filesystem to partition array with
#   filesystem: 'xfs'
#   filesystem_opts: ''
#   # Define the array raid level
#   # 0|1|4|5|6|10
#   level: '0'
#   # Define if array should be present or absent
#   state: 'present'

# mdadm_arrays:
#   - "{{ md0 }}"

# NFS config: none required/permitted

# A list of OpenHPC runtime libraries to install on compute and control nodes
openhpc_packages:
  - openmpi-gnu-ohpc
  - cfitsio
  - cfitsio-devel
  - wcslib
  - wcslib-utils
  - wcslib-devel
  - gcc-gfortran 
  - gcc-c++
  - ncurses
  - ncurses-devel
  - readline
  - readline-devel
  - python-devel
  - git-lfs
  - perf
  - perl-devel
  - gdb

extra_packages:
  - singularity
  - htop
  - screen
  - tmux
  - git
...
