---
# This name is used for the Heat stack and as a prefix for the
# cluster node hostnames.
cluster_name: openhpc

# This parameter should be set to the name of an RSA keypair you have
# uplaoded to OpenStack.
cluster_keypair: bharat

# Site-specific network configuration.
cluster_net:
  - { net: "ilab", subnet: "ilab" }
  - { net: "p3-bdn", subnet: "p3-bdn" }
  - { net: "p3-lln", subnet: "p3-lln" }

# A 3-NIC node resource in the heat templates.
cluster_nodenet_environment: "nodenet-3.yaml"

# Enable the use of config drive, for managing IP assignment on IPoIB
cluster_config_drive: true

# Multi-node application topology.  In this case we have a SLURM
# deployment formed from a login/controller node and a number of
# batch compute nodes.
cluster_groups:
  - "{{ slurm_login }}"
  - "{{ slurm_compute }}"

slurm_login:
  name: "login"
  flavor: "compute-B"
  image: "CentOS7.5-OpenHPC"
  num_nodes: 1
  user: "centos"

slurm_compute:
  name: "compute"
  flavor: "compute-A"
  image: "CentOS7.5-OpenHPC"
  num_nodes: 4
  user: "centos"

# Node group assignments for cluster roles.
# These group assignments are appended to the cluster inventory file.
# The names of these roles are cross-referenced to groups referred to
# in playbooks in the ansible/ directory.
cluster_roles:
  - name: "ceph_client"
    groups: "{{ cluster_groups }}"
  - name: "login"
    groups: [ "{{ slurm_login }}" ]
  - name: "batch"
    groups: [ "{{ slurm_compute }}" ]
  - name: "mdadm"
    groups: [ "{{ slurm_compute }}" ]
  - name: "glusterfs_server"
    groups: [ "{{ slurm_compute }}" ]
  - name: "glusterfs_client"
    groups: [ "{{ slurm_compute }}" ]
  - name: "beegfs_mgmt"
    groups: [ "{{ slurm_login }}" ]
  - name: "beegfs_mds"
    groups: [ "{{ slurm_login }}" ]
  - name: "beegfs_oss"
    groups: [ "{{ slurm_compute }}" ]
  - name: "beegfs_client"
    groups:
    - "{{ slurm_login }}"
    - "{{ slurm_compute }}" 

# Define a list of SLURM partitions to create.
openhpc_slurm_partitions: 
  - "{{ slurm_compute }}"

# A list of OpenHPC runtime libraries to install on compute and control nodes
openhpc_packages:
  - strace
  - flex
  - bison
  - blas
  - blas-devel
  - lapack
  - lapack-devel
  - cfitsio
  - cfitsio-devel
  - wcslib
  - wcslib-utils
  - wcslib-devel
  - gcc-gfortran 
  - gcc-c++
  - ncurses
  - ncurses-devel
  - readline
  - readline-devel
  - python-devel
  - boost
  - boost-devel
  - fftw
  - fftw-devel
  - hdf5
  - hdf5-devel
  - numpy
  - boost-python
  - hdf5-gnu-ohpc
  - phdf5-gnu-mvapich2-ohpc
  - gnu-compilers-ohpc
  - mvapich2-gnu-ohpc
  - openmpi-gnu-ohpc
  - imb-gnu-mvapich2-ohpc
  - imb-gnu-openmpi-ohpc
  - openblas-gnu-ohpc
  - scalapack-gnu-mvapich2-ohpc
  - python34
  - python34-devel
  - python-virtualenv
  - infiniband-diags
  - emacs
#  - git-lfs

# Gluster config
gluster_volume_name: openhpc_vol
gluster_src: "localhost:/{{ gluster_volume_name }}"
gluster_mnt: /mnt/gluster
gluster_converged_disks: true

# Admins who can login as centos user
cluster_admins:
- username: centos
  ssh_key:
  - "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDCLva2hRoqcA77XoK0t7aPmip4CmxM6SdsnJdbN4i0l72Rf6HMNH2ZR5p+eZPxaxiJgmtQ5MpquNt775lFSGVIV7oyAKDdYuLA/8KXoKje4EsCMfA9m6Eg8urjrPprelTAl7xsem1eH9c/Jk6TFbJSyMrY7kvgNo1KZ+D24F5cCSSRncy0s5GoPWEJfdnSHdjvxc97vhSPY9KwsaMdbpj+u+1WlUDeoLjMsWLPDsqnIHLwYN74f9jfTvsoQxw9Wq3dmzRvy8sQkJ1OoAeuuM4B4MKel7U6QqrhMYyIUG2pcSBBICtT2Q5V4RhnBIqQrL8w1efq+TeD/ETSuQgFBiq1 sclt100@ilab-gate.ilab.cluster"
  - "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCoHB+D6iI859Fq4Wg3iWDPf+miX+nXDx8Y8F20C5wMa7uHFYWWp0gfhMslI3ntWvlPf9J7C8AQz/kzPeuvM9oEZUkBuH6oYtYrJ9nA6+IJqT/xbQGUy56hkVVFH8Etff9CzseczzhLPOd/CEj4irLdi4aoj/5V8tqvdAbnOGwixPRPmVKVvEVpFiAlvRi+0MCaWHF2lF4BVWJsRKKGf79DFOrhfI9+BefuD9essaCOuFfhkHSxngBIjicYJKSedMFoWaOSY35820P/6RnmbKjH/mMQ0yuEr7SgLZ3gdTN2crrDj0q8+yz5MHfuxPdtj9dOYb8yP8MahbOl91qIV5BL John@Lenovo-PC"
  - "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDT7FfKdhVmT/HturOK6sLRKcqFYDnfbKWVE8Lv+KntJCBN0pGk5PECCmmA7/5JzAzfba4nmMpI20byJ0VCphzTbYFHtsZNd6Ci4NIKb5GrxP1f9RN3BYlmm1rJ5OgoXjvg0VpUiUnkLsVYkHfzUTx6tDHj8zV2DQYWGg2BtZk8YSEnigK6ppZcBuRwhY1wmKEodeXus6vik20v+PGDAlSmSlhHsYUOe8/GNhs6fZ9a9XEVHw/pPxYa3HmNGgUiJYOxSA0DLq8HzxQDoSgpMMeG2bD36DsHSnnbLNUtH3+DmveX2FxWWHRKG4IgW5O0UdLcGzrakZ6OFi/bUL0bBge7 bharat@dev-director.dev-cluster.hpc.cam.ac.uk"

# Config for Ceph mount
ceph_mount_share_name: HomeDirs
ceph_mount_path: /alaska
ceph_mount_fuse: true

# BeeGFS Configuration
beegfs_kmod_preload:
  - "mlx5_core"
  - "mlx5_ib"
  - "ib_ipoib"

# Software defined raid config
mdadm_array:
  # Define array name
  name: 'md0'
  # Define disk devices to assign to array
  devices:
    - '/dev/sdb'
    - '/dev/sdc'
    - '/dev/sdd'
  # Define filesystem to partition array with
  filesystem: 'xfs'
  # Define the array raid level
  # 0|1|4|5|6|10
  level: '0'
  # Define mountpoint for array device
  mountpoint: '/data/beegfs/beegfs_data_0'
  # Define if array should be present or absent
  state: 'present'
...
